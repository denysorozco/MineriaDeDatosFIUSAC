# -*- coding: utf-8 -*-
"""ProyectoFinalRedesNeuronalesNo.1i.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GC9VmZv1YnOr7RFexWZx9cQRP4Sfqwqc

### Universidad de San Carlos de Guatemala
### Facultad de Ingeniería
### Escuela de Estudios de Postgrado
### Curso: Mineria de Datos.
### Introducción a la Minería de Datos
### Por: Denys Fernando Orozco Escobar

# **Redes Neuronales**

Las redes neuronales son un tipo de modelo computacional que se inspira en el funcionamiento del cerebro humano, diseñado para aprender a partir de datos y tomar decisiones basadas en esos aprendizajes. A continuación, se presentan los aspectos más relevantes sobre las redes neuronales, su estructura, funcionamiento y aplicaciones.

En plataformas como Google Colab, los investigadores y desarrolladores pueden implementar y entrenar redes neuronales utilizando potentes bibliotecas como TensorFlow, Keras y PyTorch, aprovechando recursos computacionales avanzados como GPU y TPU para acelerar el proceso de aprendizaje.
"""

#Ejecute las librerías
import numpy as np
import pandas as pd
import tensorflow as tf
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

#Abre una ventana de exploración para carga de documentos .sav
from google.colab import files
# Subir archivos .sav
uploaded = files.upload()

!pip install pyreadstat
import pyreadstat

# Cargar archivos .sav
datos_2013_D , meta1 = pyreadstat.read_sav("_D_2013.sav")
datos_2014_D , meta2 = pyreadstat.read_sav("_D_2014.sav")
datos_2015_D , meta3 = pyreadstat.read_sav("_D_2015.sav")

# Visualizar los primeros registros
print(datos_2013_D.head())

# Concatenar filas
datos_d= pd.concat([datos_2013_D, datos_2014_D, datos_2015_D], ignore_index=True)
# Verificar resultado
print(datos_d.head())

#Aleatoriedad
datos_d = datos_d.sample(frac = 1, random_state = 42).reset_index(drop=True)

#Selección de la data
X = datos_d[['Cerdef','Sexo','Escodif']]
y = datos_d['Depreg']

#Datos de entrenamiento
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train = X_train.replace(' ', np.nan).dropna().astype('float32')
# Inicializando LabelEncoder
label_encoder = LabelEncoder()
# Fit and transform y_train
#Fit on all unique values to avoid unseen labels during transform
all_unique_labels = pd.concat([y_train[X_train.index], y_test[X_test.index]]).unique()
label_encoder.fit(all_unique_labels)
y_train = label_encoder.transform(y_train[X_train.index]).astype('float32')

X_test = X_test.replace(' ', np.nan).dropna().astype('float32')
# Transform y_test using the same encoder
y_test = label_encoder.transform(y_test[X_test.index]).astype('float32')

#Densidad de las capas, relu: función de activación de la capa (cambia valores negativos a positivos), introduce no linealidades
#sigmoid salida que representa la probabilidad de pertenencia a una de las clases
model = Sequential()
model.add(Dense(32, input_dim = 3, activation = 'relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss ='binary_crossentropy', optimizer ='adam', metrics =['accuracy'])

#Entrenamiento
model.fit(X_train, y_train, epochs = 50, batch_size = 200, validation_data=(X_test, y_test))

"""Probar el porcentaje de asertividad del modelo"""

loss, acc = model.evaluate(X_test, y_test)
print(acc*100)

"""**Red Neuronal No. 2**"""

datos_d = datos_d.sample(frac = 1, random_state = 42).reset_index(drop=True)

#Selección de la data
X = datos_d[['Mesocu','Sexo','Edadif']]
y = datos_d['Areag']

#Datos de entrenamiento
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

X_train, X_test, y_train, y_test  = train_test_split(X, y, test_size = 0.2, random_state = 42)

X_train = X_train.replace(' ', np.nan).dropna().astype('float32')
# Inicializando LabelEncoder
label_encoder = LabelEncoder()
# Fit and transform y_train
#Fit on all unique values to avoid unseen labels during transform
all_unique_labels = pd.concat([y_train[X_train.index], y_test[X_test.index]]).unique()
label_encoder.fit(all_unique_labels)
y_train = label_encoder.transform(y_train[X_train.index]).astype('float32')

X_test = X_test.replace(' ', np.nan).dropna().astype('float32')
# Transform y_test using the same encoder
y_test = label_encoder.transform(y_test[X_test.index]).astype('float32')

#Densidad de las capas, relu: función de activación de la capa (cambia valores negativos a positivos), introduce no linealidades
#sigmoid salida que representa la probabilidad de pertenencia a una de las clases
model = Sequential()
model.add(Dense(8, input_dim = 3, activation = 'relu'))
model.add(Dense(5, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

model.compile(loss ='binary_crossentropy', optimizer ='adam', metrics =['accuracy'])

#Entrenamiento
model.fit(X_train, y_train, epochs = 50, batch_size = 200, validation_data=(X_test, y_test))

loss, acc = model.evaluate(X_test, y_test)
print(acc*100)